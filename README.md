# neuralmon

啥是神经网络？深度潜水。
=============

[

](https://blog.roboflow.com/author/potrimba/)
[彼得鲁-波特里姆巴](https://blog.roboflow.com/author/potrimba/)
24月 2023， <>

13 分钟阅读

![](https://blog.roboflow.com/content/images/size/w1000/2023/01/Blog-Image-Template---James-2.jpg)

开发神经网络是一个活跃的研究领域，因为学术界和企业都在努力寻找更有效的方法来通过机器学习解决复杂问题。

最初，神经网络用于识别垃圾邮件等简单任务，但现在它们已扩展到更复杂的任务，如视觉搜索引擎、推荐系统、聊天机器人和医疗领域。事实上，神经网络被用于从Netflix上的电视推荐到生成文本的所有领域。

随着时间的推移，神经网络已经从可以处理有限数据的原始架构发展到在海量数据集上训练了数百万个参数的大型架构。从 YOLO 到 GPT，当今最先进的模型的核心是神经网络。

在这篇文章中，我们将讨论：

-   啥是神经网络
-   神经网络的工作原理
-   神经网络的常见架构

让我们开始吧！

啥是神经网络？
--------

神经网络是由称为"神经元"的单元组成的结构，按层排列。神经元使用数学函数来决定是否"放电"并将信息发送到另一层神经元。该架构的设计类似于人脑，其中神经元放电并在不同神经元之间建立连接。神经网络可用于解决复杂的问题，从生成图像到查找图像中的项目。

在神经网络中，数据被放入网络中并经过多层人工神经元，以产生所需的输出。每个神经元由各种组件组成，如下图所示：

![](https://lh4.googleusercontent.com/iwnAvdV4pGAOYbrnibiwIfqRW5ufbzhEN9wXA1Pw713BT8VbSGCNN2HPcMJiSr6Bba9arls2xBwBy0Qm2fCIl13KRAcCvTf9SdbiK8TPrIiMtUtpXpA5hkBGhPhYEySdPUdvbwhwv5mkPEMO3QSVO1DUNOKL87Ep5jaNZ8okHtEkGYolN_CSidFG3YAcsQ)

*神经元的表示，数学符号显示输入和输出*

神经元的特征
------

每个神经元都有四个关键特征。让我们逐一讨论。

### 输入

在学习过程中输入到模型中的特征称为输入。例如，在对象检测的情况下，输入可能是来自图像的像素值数组。

### 权重

权重用于强调对学习过程有更大影响的"特征"。在网络成功预测中出现的特征越多，代表该特征的神经元的权重就越大。权重是通过对输入值和权重矩阵应用标量乘法来计算的。例如，一个消极词对情绪分析模型的结果的影响更大，该模型的任务是识别消极词，而不是一对中性词。

### 激活功能

激活函数的主要目的是将节点的加权输入总和转换为传递到下一个隐藏层或用作最终输出的输出值。
激活函数根据神经元对网络的输入确定是否应激活神经元。这些函数使用数学运算来确定输入对预测是否重要。如果输入被认为是重要的，则该函数会"激活"神经元。

大多数激活函数都是非线性的。这允许神经网络"学习"有关数据集的特征（即不同像素如何构成图像中的特征）。如果没有非线性激活函数，神经网络将只能学习线性和仿射函数。为什么这是一个问题？这是一个问题，因为线性和仿射函数无法捕获现实世界数据中经常存在的复杂和非线性模式。

### 偏见

偏差是一个术语，用于指代在通过激活函数之前添加到输入加权和中的神经元参数。偏差通常表示为标量值，并在神经网络的训练过程中与权重一起学习。

偏置项可以通过向左或向右移动激活函数来改变神经元的输出，这可以改变输出值的范围和触发的神经元数量。这可能会对网络的整体行为产生重大影响。

神经网络的一般结构
---------

神经网络差异很大。每天，世界各地的商界和学术界人士都在试验神经网络的新配置，这些配置可以比以前的版本更好地解决给定问题。但是，一般来说，神经网络的一些特征在网络中是一致的。

下图显示了神经网络的一般结构，包括输入层、隐藏层和输出层：

![](https://lh3.googleusercontent.com/CZt0XiB2VCyVXjD5ejIQr9BkPLJ5UFYbw50tCggCnE20XDo-YpcESoOloQatXOowiId7Dt7FLGffNg4xX_dgsbEGJTYNVSL0EAmlhSvemhVx_cJ1TPkL0Ax8lawdS_GFOle407HioamQl-W-HUrQDfJYCjyh_yUgnawgMd5lcQEtstK1yGDE527P0ghCZA)

*神经网络的一般结构*

让我们谈谈这些组件中的每一个。

### 输入层

神经网络的输入层接收数据。这些数据将从图像或表格信息等来源进行处理，并简化为网络理解的结构。该层是完整神经网络架构中唯一可见的层。输入层在不执行任何计算的情况下传递原始数据。

### 隐藏层

隐藏层（如上图所示）是深度学习的支柱。它们是执行计算并从数据中提取特征的中间层。可能有多个相互连接的隐藏层，每个隐藏层负责识别数据中的不同特征。例如，在图像处理中，早期的隐藏层检测高级特征，如边缘、形状或边界，而后期的层执行更复杂的任务，如识别完整的对象，如汽车、建筑物或人。

#### **输出层**

输出层接收来自前面隐藏层的输入，并根据模型的学习信息生成最终预测。在分类/回归模型中，输出层通常具有单个节点。但是，该数字可能会有所不同，具体取决于要解决的问题的特定类型以及模型的构建方式。

神经网络架构
------

神经网络成功的关键因素之一是网络的架构，它决定了网络处理和解释信息的方式。

在本节中，我们将讨论一些最流行的神经网络架构及其应用，包括：

1.  感知器;
2.  前馈神经网络;
3.  残差网络（ResNet）;
4.  LTSM 网络;
5.  卷积神经网络和;
6.  递归神经网络。

了解不同的架构及其优势和局限性对于为给定任务选择合适的网络、实现最佳性能以及帮助您直观地了解我们如何获得当前的神经网络至关重要。

### **感知器**

感知器是最基本的神经网络架构。感知器接收多个输入，对其应用数学运算，并生成输出。

感知器接受实值输入的向量，对每个输入及其相应的权重执行线性组合，对加权输入求和，并通过激活函数传递结果。感知器单元可以组合在一起，以创建更复杂的人工神经网络架构。

### **前馈网络**

感知器模拟单个神经元的行为。当多个感知器按顺序排列并组织成层时，它形成了一个多层神经网络。

在此体系结构中，信息向前流动，从左到右，从输入层开始，经过多个隐藏层，最后到达输出层。这种类型的网络被称为前馈网络，因为信息不会在隐藏层之间回环。后面的层不提供对前一个层的反馈;学习是单向的。学习过程与感知器相同。

### **残差网络 （ResNet）**

现在您对前馈网络有所了解，您可能想知道：如何确定神经网络架构中的层数？

一个常见的误解是，网络中使用的隐藏层越多，学习过程就越好。但是，情况并非总是如此。由于梯度消失和爆炸等问题，具有多层的神经网络可能难以训练。

解决这些问题的一种方法是使用残差网络（ResNets）。与传统的前馈网络不同，ResNets 为数据流提供了替代路径，使训练更快、更容易。

ResNets的架构基于这样的理论，即通过使用身份映射从较浅的网络复制权重，可以从较浅的网络构建深度网络。来自早期层的数据被"快进"，并通过所谓的跳过连接在网络中向前传递。这些连接最初是在ResNets中引入的，以帮助解决梯度消失问题。

### 递归神经网络 （RNN）

传统的深度学习架构具有固定的输入大小，这在输入大小不固定的情况下可能是一个限制。此外，这些模型仅根据当前输入做出决策，而不考虑以前的输入。

递归神经网络 （RNN） 非常适合处理作为输入的数据序列。他们擅长 NLP 任务，例如情绪分析和垃圾邮件过滤器，以及时间序列问题（例如销售预测和股票市场预测）。RNN能够"记住"以前的输入，并使用该信息为未来的预测提供信息。

![](https://lh6.googleusercontent.com/jDz6qa1IQHGSS8X-IiAwMfq3OkKy5SUxRtXlTSWRRCxZgyI_12uFQgpEz6DyvdXxDxdy7Js6Wf5S1wJIeZVixIpGDGb5tHCDW2vcF371cGqGXnK5vKY9HhIRHhoanb_jIKlIXnNCcGKxd_mfV9dVJd47wgXqwMKB52ECqImU9ApAPnIuRbD0hkjUA_MPAw)

*RNN 细胞的表示*

在 RNN 中，顺序数据作为输入馈送。网络有一个内部隐藏状态，该状态会随着每个新的输入序列而更新。此内部隐藏状态将反馈给模型，并在每个时间戳处生成输出。在每个时间戳处，网络接收一个新的输入序列，并根据新输入及其当前隐藏状态更新此内部隐藏状态。然后，此更新的隐藏状态用于生成输出，该输出可以是预测、分类或某种其他类型的决策。

时间戳是指输入序列呈现给网络的顺序。在某些应用程序中，例如自然语言处理，时间戳可以对应于单词在句子中的位置。在其他应用程序（如时间序列预测）中，时间戳可以对应于某个时间点。

内部隐藏状态在每个时间戳反馈给模型，这意味着前一个时间步的隐藏状态被传递到当前时间步以做出预测或决策。这允许网络维护过去输入的"记忆"，并使用该信息来通知其当前输出。

### 长短期记忆网络 （LSTM）

在传统的RNN中，每个预测都完全基于以前的时间戳，并且具有有限的短期记忆。它不考虑来自更远时间的信息。为了改善这一点，我们可以通过结合"记忆"的概念来扩展递归神经网络结构。

我们可以通过在网络结构中添加称为门的组件来实现这一点。这些门允许网络记住以前时间戳的信息，使其具有更长的记忆。

![](https://lh5.googleusercontent.com/-MUMoNgn1Ja6JfxahfCsYREh8i2LAlIOhXHkTq-ImouFsFZ5xYdOuH-b_OBjxq0SBogpfu1Jl1NsHLaiRwDwOMcPCvPqLupbpmlldWXMyCIMJpYdROHQINgiilYlSAt6Dn92W4AqPX9uzfgGDjYQOZKk5HsRuhzIc5g5WRMLDUH_z9AaHUwTkssvP1Bxjw)

*LSTM 单元的表示形式*

-   **小区状态（C\_t）：**以c\_t表示的信元状态与网络的长期记忆有关。-   忘记**门：忘记**门会擦除单元格状态下不再有用的信息。它接受两个输入，当前时间戳输入 （x\_t） 和上一个单元格状态 （h\_t-1），乘以它们相应的权重矩阵，然后添加偏差。输出通过生成二进制值的激活函数传递，该函数确定是保留还是丢弃信息。-   输入门：**输入门**选择应将哪些新信息添加到单元格状态。它的操作类似于遗忘门，利用当前时间戳输入和以前的单元格状态，但区别在于使用一组不同的权重进行乘法。-   输出门：输出门的目的是从当前单元状态中识别相关信息，并将其呈现为**输出**。

### 卷积神经网络 （CNN）

卷积神经网络 （CNN） 是一种前馈神经网络，通常用于图像分析、自然语言处理和其他具有挑战性的图像分类问题等任务。

CNN由隐藏层组成，称为卷积层，构成了这些网络的基础。在图像数据中，特征是指边缘、边框、形状、纹理、对象、圆圈等小细节。

CNN的卷积层利用过滤器来检测图像数据中的这些模式，下层专注于更简单的特征，而较深的层能够检测更复杂的特征和对象。例如，在后面的层中，过滤器可能会检测特定的物体，如眼睛或耳朵，最终甚至可以检测动物，如猫和狗。

![](https://lh5.googleusercontent.com/DKSidbBVTdH6sFaomZKoqwBfQ1HbP8DhpMRa0pbmVdJxOxCOygREXSrrvzZ9lrllX-AuDWi-nZ3aUHmf5iCsq3PN8aBTRL8hrfzXtJjr6EZ3PQbDmvxmurRNh4HXXZqtB8jxU990PpACAnqe3wZgBtq_eoz3p_cDtJAP9Lty6H0wbEfg6SrydQo7rNXPrQ)

卷积神经网络的架构

向网络添加卷积层时，需要指定过滤器的数量。筛选器可以概念化为一个小矩阵，其中选择行数和列数。此功能矩阵中的值使用随机数初始化。当卷积层接收到输入数据的像素值时，滤波器会对输入矩阵的每个面块进行卷积。卷积层的输出通常通过 ReLU 激活函数传递，该函数通过将所有负值替换为零来为模型带来非线性。

池化是CNN中的关键步骤，因为它减少了计算并使模型对失真和变化更具鲁棒性。然后，完全连接的密集神经网络将使用扁平化的特征矩阵，并根据特定用例进行预测。

### 生成对抗网络 （GAN）

生成建模是无监督学习的一个子类别，其中基于从一组输入数据中发现的模式生成新的或合成的数据。生成对抗网络 （GAN） 是一种生成模型，可以通过学习输入数据中的模式来生成全新的合成数据。GAN是人工智能研究的一个流行和活跃的领域。

![](https://lh4.googleusercontent.com/c-eMFZhK7J56GwzgMcdG26XB2hEmc-RE-Dp778xumpkysAyT-lZqm-qgvDpFFS_BrWwdwilsybkwOfUTjojENCjgqwJVgS3qWPV_fiCmsenpB5Fr6Jxxe9v2fNBwfl5ieDX4ZxnIHL5z5xgCz1jmBqkCUiwu0CWtXlvEcH6eTLlgwSsQ3xGVNAjg0FXC0g)

*生成对抗网络的架构*

GAN 由两部分组成：生成器和鉴别器，它们以竞争方式工作。生成器负责根据它在训练阶段学到的特征创建合成数据。它接受随机数据作为输入，并在执行某些转换后返回生成的图像。鉴别器充当批评者，对问题域有大致的了解以及识别生成的图像的能力。

生成器创建图像，鉴别器将它们分类为假的或真实的。鉴别器返回 0 到 1 范围内的概率预测，其中 1 表示真实图像，0 表示假图像。生成器继续生成样本，鉴别器尝试区分来自训练数据的样本和生成器生成的样本。生成器接收来自鉴别器的反馈以提高其性能。

当鉴别器成功区分真假示例时，无需更改其参数。当生成器无法生成可以欺骗鉴别器的图像时，它会受到惩罚。但是，如果它成功地使鉴别器将生成的图像分类为真实图像，则表明生成器的训练进展顺利。生成器的最终目标是愚弄鉴别器，而鉴别器的目标是提高其准确性。

GAN 用于各种应用，例如预测视频中的下一帧、文本到图像生成、图像到图像转换、图像去噪等。

### 变形金刚

训练 RNN 和 LSTM 可能很慢且效率低下，尤其是对于大型测序数据和梯度消失的问题。其中一个问题是数据需要按顺序输入，这不能充分利用 GPU。

为了解决这个问题，引入了变压器，它采用编码器-解码器结构，并允许并行传递输入数据。与一次传递一个单词输入的 RNN 不同，Transformers 没有输入时间戳的概念，整个句子一起输入，所有单词的嵌入同时生成。

例如，在英语-法语翻译的情况下，变形金刚允许一次处理整个输入句子，而不是像RNN那样一次处理一个单词。

![](https://lh3.googleusercontent.com/dYTveKC--qSCuWksMR1HkPOFjaOAgTuN3NmY8CRazADbn_IWzGAQwVzDhzBdbIjwYvtwWrpuVBX7i9XTyPGuv28uTDUxrAbne-Ej-NEleFlDEQN5ZRBx0RgXqJDFwpSAEhGJQ_FmwbEVfqeSXaKNdnLqlH9ihgx5aUYj_1ISRbnLngAmHeMXp3VOsXHZEw)

变压器的架构

计算机处理数字和向量，而不是单词。为了表示单词，他们使用一种称为单词嵌入的技术，该技术将每个单词映射到称为嵌入空间的向量空间中的点。预先训练的嵌入空间用于将单词映射到向量。但是，不同上下文中的同一词可能具有不同的含义。

嵌入根据单词在句子中的位置捕获单词的上下文。通过将输入嵌入与位置编码相结合，生成的嵌入包含上下文信息。这被传递到一个编码器块，该模块包括一个多头注意力层和一个前馈层。注意力层用于确定输入句子的哪些部分对于模型要关注很重要。在训练期间，解码器被输入相应的法语句子嵌入，该嵌入由三个主要组件组成。

变压器网络中的自我注意机制为句子中的每个单词生成注意力向量，指示每个单词与同一句子中所有其他单词的相关性。然后，这些注意力向量和编码器的向量由"编码器-解码器注意力块"处理，该块评估每个单词向量之间的关系。

此块负责从英语到法语的映射。通过用变压器替换 RNN，引入了架构的重大变化。与RNN不同，变形金刚使用并行计算，并具有保留重要信息的自我注意机制，消除了RNN中发现的顺序数据处理和信息丢失问题。

### 全球通用技术总局

GPT 是一种使用生成训练的语言模型，不需要标记数据进行训练。它预测语言中单词序列的概率。到目前为止，if有三个版本：GPT-1，GPT-2和GPT-3。

GPT-1 模型经历了一个两阶段的训练过程，首先是使用大量未标记数据的无监督预训练，使用语言模型目标函数，然后使用特定于任务的数据对特定任务的模型进行监督微调。GPT-1 模型基于转换器解码器架构。

GPT-2 的主要重点是生成文本，它利用自回归方法并在输入序列上进行训练，目的是预测序列中每个点的下一个标记。该模型是使用变压器块构建的，重点是注意力机制，与BERT相比，其尺寸参数较少，但是它包含更多的变压器块（48个模块）并且可以处理更长的序列。

GPT3 的架构类似于 GPT2，但它具有更多的转换器块（96 个块），并且它是在更大的数据集上训练的。此外，GPT3 中输入句子的序列长度是 GPT2 大小的两倍，因此它是参数最多的最大的神经网络架构。

**神经网络的关键要点**
-------------

每种类型的神经网络架构都有自己的优势和局限性。

前馈神经网络广泛用于解决简单的结构化数据问题，如分类和回归。

递归神经网络在处理文本、音频和视频等顺序数据方面更有效。

最近的研究表明，使用注意力机制的变压器网络在许多领域都超过了RNN，代表了当今许多最先进模型的基础。

### 引用这篇文章：

*"[彼得鲁-波特里姆巴](https://blog.roboflow.com/author/potrimba/)。"机器人流博客，2023-1-24。https://blog.roboflow.com/what-is-a-neural-network/*


> 👆🏻 使用神经网络进行口袋妖怪类型识别的机器学习项目。这个存储库包含为《极客研究杂志》撰写的“谁是那个神经网络”文章的配套代码(https://jgeekstudies.org)
